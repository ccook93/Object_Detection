{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "\n",
    "1. Train\n",
    "2. Evaluate\n",
    "3. Visualize Accuracy & Lost\n",
    "4. Re-evaluate Model\n",
    "5. Make Predictions on the Test Set\n",
    "6. Visualize classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "from PIL.ImageDraw import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b12b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classes\n",
    "classes = ['Transformer','No-Transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Training Data and specify image directory location in system\n",
    "TRAINING_CSV_FILE = 'Data/training_data2x.csv'\n",
    "TRAINING_IMAGE_DIR = 'Images/Training'\n",
    "\n",
    "training_image_records = pd.read_csv(TRAINING_CSV_FILE)\n",
    "\n",
    "train_image_path = TRAINING_IMAGE_DIR\n",
    "\n",
    "# Initiate variables / arrays\n",
    "train_images = []\n",
    "train_targets = []\n",
    "train_labels = []\n",
    "\n",
    "# Iterrate through rows in the csv file\n",
    "for index, row in training_image_records.iterrows():\n",
    "    \n",
    "    # Define a row to include the following headers from the csv\n",
    "    (filename, width, height, class_name, xmin, ymin, xmax, ymax) = row\n",
    "    # Specify training image path (Create string) - joining the filename in csv and the path\n",
    "    train_image_fullpath = os.path.join(train_image_path, filename)\n",
    "    # Load the image\n",
    "    train_img = keras.preprocessing.image.load_img(train_image_fullpath, target_size=(height, width))\n",
    "    # Create an array of the image\n",
    "    train_img_arr = keras.preprocessing.image.img_to_array(train_img)\n",
    "    \n",
    "    # Define xmin, ymin, xmax, and ymax\n",
    "    xmin = round(xmin/ width, 2)\n",
    "    ymin = round(ymin/ height, 2)\n",
    "    xmax = round(xmax/ width, 2)\n",
    "    ymax = round(ymax/ height, 2)\n",
    "    \n",
    "    # Append this information in their respective arrays initiated for the 'For' loop\n",
    "    train_images.append(train_img_arr)\n",
    "    train_targets.append((xmin, ymin, xmax, ymax))\n",
    "    train_labels.append(classes.index(class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat process as described above but with TEST CSV and images\n",
    "TEST_CSV_FILE = 'Data/test_data2.csv'\n",
    "TEST_IMAGE_DIR = 'Images/Test'\n",
    "\n",
    "test_image_records = pd.read_csv(TEST_CSV_FILE)\n",
    "\n",
    "test_image_path = TEST_IMAGE_DIR\n",
    "\n",
    "test_images = []\n",
    "test_targets = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in test_image_records.iterrows():\n",
    "\n",
    "    (filename, width, height, class_name, xmin, ymin, xmax, ymax) = row\n",
    "\n",
    "    test_image_fullpath = os.path.join(test_image_path, filename)\n",
    "    test_img = keras.preprocessing.image.load_img(test_image_fullpath, target_size=(height, width))\n",
    "    test_img_arr = keras.preprocessing.image.img_to_array(test_img)\n",
    "\n",
    "\n",
    "    xmin = round(xmin/ width, 2)\n",
    "    ymin = round(ymin/ height, 2)\n",
    "    xmax = round(xmax/ width, 2)\n",
    "    ymax = round(ymax/ height, 2)\n",
    "\n",
    "    test_images.append(test_img_arr)\n",
    "    test_targets.append((xmin, ymin, xmax, ymax))\n",
    "    test_labels.append(classes.index(class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine appended arraya to np.arrays\n",
    "train_images = np.array(train_images)\n",
    "train_targets = np.array(train_targets)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_targets = np.array(test_targets)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying width and height of images & number of classes in this classification\n",
    "width = 518\n",
    "height = 388\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8dd9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 06:15:28.406850: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Specified convolution layers\n",
    "#create the common input layer\n",
    "input_shape = (height, width, 3)\n",
    "input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "#create the base layers\n",
    "base_layers = layers.experimental.preprocessing.Rescaling(1./255, name='bl_1')(input_layer)\n",
    "base_layers = layers.Conv2D(16, 3, padding='same', activation='relu', name='bl_2')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_3')(base_layers)\n",
    "base_layers = layers.Conv2D(32, 3, padding='same', activation='relu', name='bl_4')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_5')(base_layers)\n",
    "base_layers = layers.Conv2D(64, 3, padding='same', activation='relu', name='bl_6')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_7')(base_layers)\n",
    "base_layers = layers.Flatten(name='bl_8')(base_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "894e4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the classifier branch\n",
    "classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
    "classifier_branch = layers.Dense(num_classes, name='cl_head')(classifier_branch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c20f3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
    "locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
    "locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
    "locator_branch = layers.Dense(4, activation='sigmoid', name='bb_head')(locator_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "097a5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input_layer,\n",
    "           outputs=[classifier_branch,locator_branch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "481566e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"cl_head\":tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "   \"bb_head\":tf.keras.losses.MSE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2135a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses, optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "021b0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTargets = {\n",
    "    \"cl_head\": train_labels,\n",
    "    \"bb_head\": train_targets\n",
    "}\n",
    "testTargets = {\n",
    "    \"cl_head\": test_labels,\n",
    "    \"bb_head\": test_targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69ce1897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 40s 514ms/step - loss: 1.3499 - cl_head_loss: 1.2513 - bb_head_loss: 0.0986 - cl_head_accuracy: 0.5767 - bb_head_accuracy: 0.3233 - val_loss: 0.8802 - val_cl_head_loss: 0.6934 - val_bb_head_loss: 0.1867 - val_cl_head_accuracy: 0.4000 - val_bb_head_accuracy: 0.1600\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 38s 502ms/step - loss: 0.7942 - cl_head_loss: 0.7073 - bb_head_loss: 0.0869 - cl_head_accuracy: 0.5900 - bb_head_accuracy: 0.3633 - val_loss: 0.7892 - val_cl_head_loss: 0.7140 - val_bb_head_loss: 0.0752 - val_cl_head_accuracy: 0.2200 - val_bb_head_accuracy: 0.1600\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 37s 499ms/step - loss: 0.7517 - cl_head_loss: 0.6718 - bb_head_loss: 0.0799 - cl_head_accuracy: 0.6300 - bb_head_accuracy: 0.3600 - val_loss: 0.8049 - val_cl_head_loss: 0.6985 - val_bb_head_loss: 0.1065 - val_cl_head_accuracy: 0.5100 - val_bb_head_accuracy: 0.1500\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 37s 500ms/step - loss: 0.5840 - cl_head_loss: 0.5244 - bb_head_loss: 0.0596 - cl_head_accuracy: 0.7467 - bb_head_accuracy: 0.3233 - val_loss: 0.7772 - val_cl_head_loss: 0.7029 - val_bb_head_loss: 0.0743 - val_cl_head_accuracy: 0.5700 - val_bb_head_accuracy: 0.1500\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 37s 497ms/step - loss: 0.3282 - cl_head_loss: 0.2954 - bb_head_loss: 0.0328 - cl_head_accuracy: 0.8633 - bb_head_accuracy: 0.4600 - val_loss: 0.8304 - val_cl_head_loss: 0.7602 - val_bb_head_loss: 0.0701 - val_cl_head_accuracy: 0.6000 - val_bb_head_accuracy: 0.1200\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 5\n",
    "\n",
    "history = model.fit(train_images, trainTargets,\n",
    "            validation_data=(test_images, testTargets),\n",
    "             batch_size=4,\n",
    "             epochs=training_epochs,\n",
    "             shuffle=True,\n",
    "             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12efe56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
